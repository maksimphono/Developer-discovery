{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455cc887-8373-451d-8c96-a66b7530486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/trukhinmaksim/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac85e37c-719b-47b1-ad1d-d30439b5de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from random import sample, seed as randomSeed\n",
    "from collections import defaultdict\n",
    "from numpy import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b374dd4-9b71-43d2-8770-d21b16bcc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.CacheAdapter import JSONAdapter, JSONMultiFileAdapter, EXP_END_OF_DATA\n",
    "from src.utils.DatasetManager import ProjectsDatasetManager\n",
    "from src.utils.validators import projectDataIsSufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58adadd7-d7f4-427c-a97d-8720bbaf1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54ebeff-53b3-4437-92a9-78ede1a03b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer\n",
    "from src.utils.AutoTuner import AutoTuner, Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b772eb3-0407-4b1a-84ea-66c9710f9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FILE_NAME = \"cache__02-04-2025__(good)_{0}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5efae60e-04ba-4278-816a-35a98e781082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatternData(data : dict[str, list]) -> np.array(dict):\n",
    "    # takes in data in form of dict, where each key is a user id and each value is a list of that user's projects\n",
    "    # returns just flat list of these projects \n",
    "    result = []\n",
    "\n",
    "    for projectsArray in data.values():\n",
    "        for project in projectsArray:\n",
    "            result.append(project)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39bb961c-3354-44d7-b365-b364502962bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adapter to load data from the cache files\n",
    "\n",
    "# TODO: place implementation of the 'Corpus' class into a separate file\n",
    "class Corpus:\n",
    "    # base class for every data corpus, that will be used by model\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __iter__(self):\n",
    "        pass\n",
    "    def __getitem__(self, index : int):\n",
    "        pass\n",
    "\n",
    "class CacheCorpus(Corpus):\n",
    "    def __init__(self, manager, cacheFileNameTemplate = CACHE_FILE_NAME, limit = float(\"inf\")):\n",
    "        self.cacheFileNameTemplate = cacheFileNameTemplate\n",
    "        self.manager = manager # manager is needed not only for interaction with adapter, but also if I want to use unpreprocessed dataset and preprocess it on the way\n",
    "        self.limit = limit\n",
    "        self.resetOnIter = False\n",
    "\n",
    "    #def __iter__(self):\n",
    "    #    return self\n",
    "\n",
    "    def __iter__(self):\n",
    "        # will feed preprocessed projects data as TaggedDocument instances one by one\n",
    "        cacheFileName = self.cacheFileNameTemplate\n",
    "        tempStorage = [] # temporary storage for data, that was read from files\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                while len(tempStorage) >= 1:\n",
    "                    doc = tempStorage[0]\n",
    "                    yield TaggedDocument(words = doc[\"tokens\"], tags = doc[\"tags\"])\n",
    "                    i += 1\n",
    "                    if i >= self.limit:\n",
    "                        raise EXP_END_OF_DATA\n",
    "\n",
    "                    tempStorage = tempStorage[1:]\n",
    "\n",
    "                #self.manager.cacheAdapter.collectionName = cacheFileName.format(i)\n",
    "                data = flatternData(self.manager.fromCache())\n",
    "                tempStorage.extend(data)\n",
    "\n",
    "            except EXP_END_OF_DATA:\n",
    "            # no data left\n",
    "                break\n",
    "\n",
    "        i = 0\n",
    "        tempStorage.clear()\n",
    "        self.manager.cacheAdapter.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84383a7b-6937-4bf2-9cce-74305b3b0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5f4d69-6c5e-432d-91fc-d1573f105b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXP_CORPUS_IS_NONE(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"'Model.corpus' object must be an iterable structure, inherited from 'Corpus' class!\")\n",
    "\n",
    "class EXP_MANAGER_IS_NONE(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"'Model.manager' object must be a DatasetManager instance!\")\n",
    "\n",
    "\n",
    "class Model(gensim.models.doc2vec.Doc2Vec):\n",
    "    manager = None\n",
    "    corpus = None\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, **kwargs):\n",
    "        model = Model(\n",
    "                vector_size = VECTOR_SIZE,\n",
    "                dm_dbow_mode = \"DM\", \n",
    "                alpha_init = ALPHA_INIT,\n",
    "                alpha_final = ALPHA_FINAL,\n",
    "                **kwargs\n",
    "            )\n",
    "        cls.manager.cacheAdapter.reset()\n",
    "        cls.manager.clearData()\n",
    "        model.corpus = cls.corpus\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def __init__(self, dm_dbow_mode = \"DM\", pretrain_w2v = False, alpha_init = 0.05, alpha_final = 0.001, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.corpus = None # corpus is an iterator(iterable class object), that will be used in \"train\" method of Doc2Vec model for data extraction\n",
    "        self.alphaInit = alpha_init\n",
    "        self.alphaFinal = alpha_final\n",
    "        self.dmDbowMode = dm_dbow_mode\n",
    "        self.pretrainW2V = pretrain_w2v\n",
    "    \n",
    "    def train(self):\n",
    "        # will build vocabulary and train the model on corpus (corpus will be fed by corpus)\n",
    "        \n",
    "        if not isinstance(self.corpus, Corpus): raise EXP_CORPUS_IS_NONE\n",
    "        if not isinstance(self.manager, ProjectsDatasetManager): raise EXP_MANAGER_IS_NONE\n",
    "\n",
    "        start = time()\n",
    "        self.build_vocab(self.corpus)\n",
    "        print(f\"Vocabulary built in {time() - start} s\")\n",
    "\n",
    "        if self.dmDbowMode != \"DM+DBOW\":\n",
    "            start = time()\n",
    "            super().train(\n",
    "                self.corpus, \n",
    "                total_examples = self.corpus_count, \n",
    "                epochs = self.epochs,\n",
    "                start_alpha = self.alphaInit,\n",
    "                end_alpha = self.alphaFinal\n",
    "            )\n",
    "            print(f\"Training is completed in {time() - start} s\")\n",
    "        else:\n",
    "            # combine DM and DBOW\n",
    "            pass\n",
    "\n",
    "    def assess(self, sampleNum = 5, silent = False, format = \"full\", random_state = None):\n",
    "        # simple test of model performance\n",
    "        # take multiple documents from the training corpus and tries to find simillar in the dataset\n",
    "        # format = \"full\" | \"mean\"\n",
    "\n",
    "        log = lambda s: print(s) if not silent else None\n",
    "        #performanceGrageScale = {50 : \"Random\", 60 : \"Poor\", 70 : \"Bad\", 80 : \"Medium\", 92 : \"Optimal\", 97 : \"Perfect\"}\n",
    "        totalDocuments = self.corpus_count\n",
    "        if random_state != None: randomSeed(random_state)\n",
    "        indexes = sample(range(totalDocuments), sampleNum)\n",
    "        print(indexes)\n",
    "        if format == \"full\":\n",
    "            stats = {}\n",
    "\n",
    "        i = 0\n",
    "        avgPerformances = []\n",
    "\n",
    "        for doc in self.corpus:\n",
    "            if i >= totalDocuments: break\n",
    "            if i in indexes:\n",
    "                vector = self.infer_vector(doc.words)\n",
    "                sims = defaultdict(lambda: 0, self.dv.most_similar([vector], topn = totalDocuments))\n",
    "\n",
    "                log(f\"Assessing document {i} ({doc.tags}). Similarities by tags:\")\n",
    "                if format == \"full\":\n",
    "                    stats[i] = {\n",
    "                        \"similarities by tags\" : {},\n",
    "                        \"average\" : 0\n",
    "                    }\n",
    "\n",
    "                for tag in doc.tags:\n",
    "                    if format == \"full\":\n",
    "                        stats[i][\"similarities by tags\"][tag] = sims[tag]\n",
    "                    log(f\"  {tag} : {sims[tag]}\")\n",
    "\n",
    "                avgPerformances.append(mean([sims[tag] for tag in doc.tags]))\n",
    "                log(f\"\\n  Average similarity value: {avgPerformances[-1]}\\n\")\n",
    "                if format == \"full\":\n",
    "                    stats[i][\"average\"] = avgPerformances[-1]\n",
    "            i += 1\n",
    "\n",
    "        if format == \"full\":\n",
    "            stats[\"Average accuracy\"] = mean(avgPerformances)\n",
    "            log(f\"Average accuracy: {stats['Average accuracy']}\")\n",
    "\n",
    "            return stats\n",
    "        else:\n",
    "            return mean(avgPerformances)\n",
    "\n",
    "    def evaluate(self):\n",
    "        # will train the model on upon-selected set of parameters and test it's performance\n",
    "        self.train()\n",
    "\n",
    "        result = self.assess(6, silent = True, format = \"mean\", random_state = 42)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "584ed1f5-c3a9-4aa4-8368-22fec165fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = JSONMultiFileAdapter(CACHE_FILE_NAME)\n",
    "#ProjectsDatasetManager.usersCollection = usersCollection\n",
    "#ProjectsDatasetManager.projectsCollection = projectsCollection\n",
    "manager = ProjectsDatasetManager(50, cacheAdapter = adapter)\n",
    "corpus = CacheCorpus(manager, limit = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9150ae5-5a1a-4edb-be81-93c16fa102be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# finetunning is done by twicking model parameters\\nmodel = Model(\\n    vector_size =  VECTOR_SIZE, \\n    window =       WINDOW_SIZE, \\n    min_count =    WORD_MIN_COUNT, \\n    epochs =       EPOCHS_NUMBER, \\n    dm_dbow_mode = DM_DBOW_MODE,\\n    negative =     NEGATIVE_SAMPLES_AMOUNT,\\n    sample =       SUBSAMPLING_THRESHOLD,\\n    alpha_init =   ALPHA_INIT,\\n    alpha_final =  ALPHA_FINAL\\n)\\nmodel.corpus = CacheCorpus(manager, limit = 50)\\n#model.assess()\\n#model.train()\\nprint(model.corpus_count)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating model\n",
    "\n",
    "VECTOR_SIZE = 7\n",
    "EPOCHS_NUMBER = 1\n",
    "WORD_MIN_COUNT = 5\n",
    "WINDOW_SIZE = 7\n",
    "NEGATIVE_SAMPLES_AMOUNT = 6\n",
    "SUBSAMPLING_THRESHOLD = 1e-5\n",
    "ALPHA_INIT = 0.05\n",
    "ALPHA_FINAL = 0.00001\n",
    "DM_DBOW_MODE = \"DM\" # \"DBOW\" \"DM+DBOW\"\n",
    "\"\"\"\n",
    "# finetunning is done by twicking model parameters\n",
    "model = Model(\n",
    "    vector_size =  VECTOR_SIZE, \n",
    "    window =       WINDOW_SIZE, \n",
    "    min_count =    WORD_MIN_COUNT, \n",
    "    epochs =       EPOCHS_NUMBER, \n",
    "    dm_dbow_mode = DM_DBOW_MODE,\n",
    "    negative =     NEGATIVE_SAMPLES_AMOUNT,\n",
    "    sample =       SUBSAMPLING_THRESHOLD,\n",
    "    alpha_init =   ALPHA_INIT,\n",
    "    alpha_final =  ALPHA_FINAL\n",
    ")\n",
    "model.corpus = CacheCorpus(manager, limit = 50)\n",
    "#model.assess()\n",
    "#model.train()\n",
    "print(model.corpus_count)\n",
    "\"\"\"\n",
    "#model.build_vocab(documentsCorpus)\n",
    "#model.train(documentsCorpus, total_examples = model.corpus_count, epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402653a4-ee31-443e-ad6f-c7f2187edbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 15:22:31,142 : INFO : Model lifecycle event {'params': 'Model<dm/m,d7,n5,w5,mc7,s1e-05,t3>', 'datetime': '2025-04-09T15:22:31.141942', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'created'}\n",
      "2025-04-09 15:22:31,145 : INFO : Model lifecycle event {'params': 'Model<dm/m,d7,n5,w5,mc7,s1e-05,t3>', 'datetime': '2025-04-09T15:22:31.145471', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'created'}\n",
      "2025-04-09 15:22:31,146 : INFO : collecting all words and their counts\n",
      "2025-04-09 15:22:31,151 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2025-04-09 15:22:31,154 : WARNING : More unique tags (360) than documents (50).\n",
      "2025-04-09 15:22:31,160 : INFO : collected 307 word types and 360 unique tags from a corpus of 50 examples and 448 words\n",
      "2025-04-09 15:22:31,161 : INFO : Creating a fresh vocabulary\n",
      "2025-04-09 15:22:31,164 : INFO : Model lifecycle event {'msg': 'effective_min_count=7 retains 4 unique words (1.30% of original 307, drops 303)', 'datetime': '2025-04-09T15:22:31.164202', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2025-04-09 15:22:31,165 : INFO : Model lifecycle event {'msg': 'effective_min_count=7 leaves 34 word corpus (7.59% of original 448, drops 414)', 'datetime': '2025-04-09T15:22:31.165311', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2025-04-09 15:22:31,166 : INFO : deleting the raw counts dictionary of 307 items\n",
      "2025-04-09 15:22:31,167 : INFO : sample=1e-05 downsamples 4 most-common words\n",
      "2025-04-09 15:22:31,168 : INFO : Model lifecycle event {'msg': 'downsampling leaves estimated 0.21419881156618426 word corpus (0.6%% of prior 34)', 'datetime': '2025-04-09T15:22:31.168803', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2025-04-09 15:22:31,170 : INFO : estimated required memory for 4 words and 7 dimensions: 84304 bytes\n",
      "2025-04-09 15:22:31,171 : INFO : resetting layer weights\n",
      "2025-04-09 15:22:31,174 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2025-04-09 15:22:31,176 : INFO : Model lifecycle event {'msg': 'training model with 3 workers on 4 vocabulary and 7 features, using sg=0 hs=0 sample=1e-05 negative=5 window=5 shrink_windows=True', 'datetime': '2025-04-09T15:22:31.176072', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'train'}\n",
      "2025-04-09 15:22:31,184 : INFO : EPOCH 0: training on 448 raw words (470 effective words) took 0.0s, 84038 effective words/s\n",
      "2025-04-09 15:22:31,191 : INFO : EPOCH 1: training on 448 raw words (470 effective words) took 0.0s, 95360 effective words/s\n",
      "2025-04-09 15:22:31,197 : INFO : EPOCH 2: training on 448 raw words (470 effective words) took 0.0s, 109733 effective words/s\n",
      "2025-04-09 15:22:31,204 : INFO : EPOCH 3: training on 448 raw words (470 effective words) took 0.0s, 90560 effective words/s\n",
      "2025-04-09 15:22:31,212 : INFO : EPOCH 4: training on 448 raw words (470 effective words) took 0.0s, 79900 effective words/s\n",
      "2025-04-09 15:22:31,213 : INFO : Model lifecycle event {'msg': 'training on 2240 raw words (2350 effective words) took 0.0s, 65106 effective words/s', 'datetime': '2025-04-09T15:22:31.213066', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'train'}\n",
      "2025-04-09 15:22:31,234 : INFO : Model lifecycle event {'params': 'Model<dm/m,d7,n9,w6,mc7,s5.01249e-06,t3>', 'datetime': '2025-04-09T15:22:31.234000', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'created'}\n",
      "2025-04-09 15:22:31,235 : INFO : collecting all words and their counts\n",
      "2025-04-09 15:22:31,239 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2025-04-09 15:22:31,241 : WARNING : More unique tags (360) than documents (50).\n",
      "2025-04-09 15:22:31,244 : INFO : collected 307 word types and 360 unique tags from a corpus of 50 examples and 448 words\n",
      "2025-04-09 15:22:31,245 : INFO : Creating a fresh vocabulary\n",
      "2025-04-09 15:22:31,246 : INFO : Model lifecycle event {'msg': 'effective_min_count=7 retains 4 unique words (1.30% of original 307, drops 303)', 'datetime': '2025-04-09T15:22:31.246318', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2025-04-09 15:22:31,247 : INFO : Model lifecycle event {'msg': 'effective_min_count=7 leaves 34 word corpus (7.59% of original 448, drops 414)', 'datetime': '2025-04-09T15:22:31.247317', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2025-04-09 15:22:31,248 : INFO : deleting the raw counts dictionary of 307 items\n",
      "2025-04-09 15:22:31,249 : INFO : sample=5.01249e-06 downsamples 4 most-common words\n",
      "2025-04-09 15:22:31,250 : INFO : Model lifecycle event {'msg': 'downsampling leaves estimated 0.151369394997344 word corpus (0.4%% of prior 34)', 'datetime': '2025-04-09T15:22:31.250508', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2025-04-09 15:22:31,251 : INFO : estimated required memory for 4 words and 7 dimensions: 84304 bytes\n",
      "2025-04-09 15:22:31,252 : INFO : resetting layer weights\n",
      "2025-04-09 15:22:31,254 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2025-04-09 15:22:31,255 : INFO : Model lifecycle event {'msg': 'training model with 3 workers on 4 vocabulary and 7 features, using sg=0 hs=0 sample=5.012494775682321e-06 negative=9 window=6 shrink_windows=True', 'datetime': '2025-04-09T15:22:31.255325', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'train'}\n",
      "2025-04-09 15:22:31,264 : INFO : EPOCH 0: training on 448 raw words (470 effective words) took 0.0s, 80270 effective words/s\n",
      "2025-04-09 15:22:31,277 : INFO : EPOCH 1: training on 448 raw words (470 effective words) took 0.0s, 40764 effective words/s\n",
      "2025-04-09 15:22:31,288 : INFO : EPOCH 2: training on 448 raw words (470 effective words) took 0.0s, 65462 effective words/s\n",
      "2025-04-09 15:22:31,298 : INFO : EPOCH 3: training on 448 raw words (470 effective words) took 0.0s, 70106 effective words/s\n",
      "2025-04-09 15:22:31,304 : INFO : EPOCH 4: training on 448 raw words (470 effective words) took 0.0s, 104060 effective words/s\n",
      "2025-04-09 15:22:31,312 : INFO : EPOCH 5: training on 448 raw words (470 effective words) took 0.0s, 91486 effective words/s\n",
      "2025-04-09 15:22:31,319 : INFO : EPOCH 6: training on 448 raw words (470 effective words) took 0.0s, 100522 effective words/s\n",
      "2025-04-09 15:22:31,325 : INFO : EPOCH 7: training on 448 raw words (471 effective words) took 0.0s, 106634 effective words/s\n",
      "2025-04-09 15:22:31,334 : INFO : EPOCH 8: training on 448 raw words (470 effective words) took 0.0s, 74687 effective words/s\n",
      "2025-04-09 15:22:31,340 : INFO : EPOCH 9: training on 448 raw words (470 effective words) took 0.0s, 101995 effective words/s\n",
      "2025-04-09 15:22:31,347 : INFO : EPOCH 10: training on 448 raw words (471 effective words) took 0.0s, 93204 effective words/s\n",
      "2025-04-09 15:22:31,354 : INFO : EPOCH 11: training on 448 raw words (470 effective words) took 0.0s, 94671 effective words/s\n",
      "2025-04-09 15:22:31,359 : INFO : EPOCH 12: training on 448 raw words (470 effective words) took 0.0s, 112225 effective words/s\n",
      "2025-04-09 15:22:31,360 : INFO : Model lifecycle event {'msg': 'training on 5824 raw words (6112 effective words) took 0.1s, 58521 effective words/s', 'datetime': '2025-04-09T15:22:31.360618', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary built in 0.027740955352783203 s\n",
      "Training is completed in 0.03887772560119629 s\n",
      "[40, 7, 1, 17, 15, 14]\n",
      "Vocabulary built in 0.018922805786132812 s\n",
      "Training is completed in 0.10718393325805664 s\n",
      "[40, 7, 1, 17, 15, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 15:22:31,469 : INFO : Model lifecycle event {'params': 'Model<dm/m,d7,n11,w5,mc9,s1.12034e-06,t3>', 'datetime': '2025-04-09T15:22:31.469363', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'created'}\n",
      "2025-04-09 15:22:31,470 : INFO : collecting all words and their counts\n",
      "2025-04-09 15:22:31,473 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2025-04-09 15:22:31,476 : WARNING : More unique tags (360) than documents (50).\n",
      "2025-04-09 15:22:31,481 : INFO : collected 307 word types and 360 unique tags from a corpus of 50 examples and 448 words\n",
      "2025-04-09 15:22:31,487 : INFO : Creating a fresh vocabulary\n",
      "2025-04-09 15:22:31,488 : INFO : Model lifecycle event {'msg': 'effective_min_count=9 retains 1 unique words (0.33% of original 307, drops 306)', 'datetime': '2025-04-09T15:22:31.488706', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2025-04-09 15:22:31,489 : INFO : Model lifecycle event {'msg': 'effective_min_count=9 leaves 13 word corpus (2.90% of original 448, drops 435)', 'datetime': '2025-04-09T15:22:31.489326', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2025-04-09 15:22:31,490 : INFO : deleting the raw counts dictionary of 307 items\n",
      "2025-04-09 15:22:31,493 : INFO : sample=1.12034e-06 downsamples 1 most-common words\n",
      "2025-04-09 15:22:31,498 : INFO : Model lifecycle event {'msg': 'downsampling leaves estimated 0.013774545537774947 word corpus (0.1%% of prior 13)', 'datetime': '2025-04-09T15:22:31.498070', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}\n",
      "2025-04-09 15:22:31,500 : INFO : estimated required memory for 1 words and 7 dimensions: 82636 bytes\n",
      "2025-04-09 15:22:31,505 : INFO : resetting layer weights\n",
      "2025-04-09 15:22:31,508 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2025-04-09 15:22:31,510 : INFO : Model lifecycle event {'msg': 'training model with 3 workers on 1 vocabulary and 7 features, using sg=0 hs=0 sample=1.1203377583638443e-06 negative=11 window=5 shrink_windows=True', 'datetime': '2025-04-09T15:22:31.510023', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'train'}\n",
      "2025-04-09 15:22:31,525 : INFO : EPOCH 0: training on 448 raw words (470 effective words) took 0.0s, 70723 effective words/s\n",
      "2025-04-09 15:22:31,543 : INFO : EPOCH 1: training on 448 raw words (470 effective words) took 0.0s, 35072 effective words/s\n",
      "2025-04-09 15:22:31,558 : INFO : EPOCH 2: training on 448 raw words (470 effective words) took 0.0s, 39513 effective words/s\n",
      "2025-04-09 15:22:31,574 : INFO : EPOCH 3: training on 448 raw words (470 effective words) took 0.0s, 42399 effective words/s\n",
      "2025-04-09 15:22:31,586 : INFO : EPOCH 4: training on 448 raw words (470 effective words) took 0.0s, 50214 effective words/s\n",
      "2025-04-09 15:22:31,595 : INFO : EPOCH 5: training on 448 raw words (470 effective words) took 0.0s, 70462 effective words/s\n",
      "2025-04-09 15:22:31,602 : INFO : EPOCH 6: training on 448 raw words (470 effective words) took 0.0s, 84075 effective words/s\n",
      "2025-04-09 15:22:31,609 : INFO : EPOCH 7: training on 448 raw words (470 effective words) took 0.0s, 96201 effective words/s\n",
      "2025-04-09 15:22:31,616 : INFO : EPOCH 8: training on 448 raw words (470 effective words) took 0.0s, 98937 effective words/s\n",
      "2025-04-09 15:22:31,624 : INFO : EPOCH 9: training on 448 raw words (470 effective words) took 0.0s, 73316 effective words/s\n",
      "2025-04-09 15:22:31,631 : INFO : EPOCH 10: training on 448 raw words (470 effective words) took 0.0s, 91911 effective words/s\n",
      "2025-04-09 15:22:31,638 : INFO : EPOCH 11: training on 448 raw words (470 effective words) took 0.0s, 96231 effective words/s\n",
      "2025-04-09 15:22:31,644 : INFO : EPOCH 12: training on 448 raw words (470 effective words) took 0.0s, 105557 effective words/s\n",
      "2025-04-09 15:22:31,651 : INFO : EPOCH 13: training on 448 raw words (470 effective words) took 0.0s, 111506 effective words/s\n",
      "2025-04-09 15:22:31,651 : INFO : Model lifecycle event {'msg': 'training on 6272 raw words (6580 effective words) took 0.1s, 46763 effective words/s', 'datetime': '2025-04-09T15:22:31.651943', 'gensim': '4.3.3', 'python': '3.11.11 (main, Jan 14 2025, 05:22:51) [GCC 12.2.0]', 'platform': 'Linux-6.13.8-200.fc41.x86_64-x86_64-with-glibc2.36', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary built in 0.03801369667053223 s\n",
      "Training is completed in 0.143829345703125 s\n",
      "[40, 7, 1, 17, 15, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          fun: -0.10587916989820173\n",
       "            x: [5, 7, 5, 5, 1e-05]\n",
       "    func_vals: [-1.059e-01 -1.059e-01 -1.059e-01]\n",
       "      x_iters: [[5, 7, 5, 5, 1e-05], [6, 7, 13, 9, 5.012494775682321e-06], [5, 9, 14, 11, 1.1203377583638443e-06]]\n",
       "       models: [GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=1608637542), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
       "                                        n_restarts_optimizer=2, noise='gaussian',\n",
       "                                        normalize_y=True, random_state=1608637542)]\n",
       "        space: Space([Integer(low=5, high=6, prior='uniform', transform='normalize'),\n",
       "                      Integer(low=7, high=9, prior='uniform', transform='normalize'),\n",
       "                      Integer(low=5, high=15, prior='uniform', transform='normalize'),\n",
       "                      Integer(low=5, high=11, prior='uniform', transform='normalize'),\n",
       "                      Real(low=1e-06, high=1e-05, prior='uniform', transform='normalize')])\n",
       " random_state: RandomState(MT19937)\n",
       "        specs:     args:                    func: <bound method AutoTuner.objective of <src.utils.AutoTuner.AutoTuner object at 0x7f8a4b5323d0>>\n",
       "                                      dimensions: Space([Integer(low=5, high=6, prior='uniform', transform='normalize'),\n",
       "                                                         Integer(low=7, high=9, prior='uniform', transform='normalize'),\n",
       "                                                         Integer(low=5, high=15, prior='uniform', transform='normalize'),\n",
       "                                                         Integer(low=5, high=11, prior='uniform', transform='normalize'),\n",
       "                                                         Real(low=1e-06, high=1e-05, prior='uniform', transform='normalize')])\n",
       "                                  base_estimator: GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1], nu=2.5),\n",
       "                                                                           n_restarts_optimizer=2, noise='gaussian',\n",
       "                                                                           normalize_y=True, random_state=1608637542)\n",
       "                                         n_calls: 2\n",
       "                                 n_random_starts: None\n",
       "                                n_initial_points: 1\n",
       "                         initial_point_generator: random\n",
       "                                        acq_func: gp_hedge\n",
       "                                   acq_optimizer: auto\n",
       "                                              x0: [5, 7, 5, 5, 1e-05]\n",
       "                                              y0: [-0.10587916989820173]\n",
       "                                    random_state: RandomState(MT19937)\n",
       "                                         verbose: False\n",
       "                                        callback: None\n",
       "                                        n_points: 10000\n",
       "                            n_restarts_optimizer: 5\n",
       "                                              xi: 0.01\n",
       "                                           kappa: 1.96\n",
       "                                          n_jobs: 1\n",
       "                                model_queue_size: None\n",
       "                                space_constraint: None\n",
       "               function: base_minimize"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autotunning model parameters\n",
    "\n",
    "def createModel(**kwargs):\n",
    "    model = Model(\n",
    "                vector_size = VECTOR_SIZE,\n",
    "                dm_dbow_mode = \"DM\", \n",
    "                alpha_init = ALPHA_INIT,\n",
    "                alpha_final = ALPHA_FINAL,\n",
    "                **kwargs\n",
    "            )\n",
    "    manager.cacheAdapter.reset()\n",
    "    manager.clearData()\n",
    "    model.corpus = corpus\n",
    "\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "tuner = AutoTuner(createModel, [\n",
    "    Param(_name = \"window\",    _type = Integer,  _range = (5, 10),      _initial = 7),\n",
    "    Param(_name = \"min_count\", _type = Integer,  _range = (7, 12),      _initial = 7),\n",
    "    Param(_name = \"epochs\",    _type = Integer,  _range = (25, 45),     _initial = 25),\n",
    "    Param(_name = \"negative\",  _type = Integer,  _range = (5, 11),      _initial = 5),\n",
    "    Param(_name = \"sample\",    _type = Real,     _range = (1e-6, 1e-5), _initial = 1e-5),\n",
    "])\n",
    "\"\"\"\n",
    "tuner = AutoTuner(createModel, [\n",
    "    Param(_name = \"window\",    _type = Integer,  _range = (5, 6),      _initial = 5),\n",
    "    Param(_name = \"min_count\", _type = Integer,  _range = (7, 9),      _initial = 7),\n",
    "    Param(_name = \"epochs\",    _type = Integer,  _range = (5, 15),     _initial = 5),\n",
    "    Param(_name = \"negative\",  _type = Integer,  _range = (5, 11),      _initial = 5),\n",
    "    Param(_name = \"sample\",    _type = Real,     _range = (1e-6, 1e-5), _initial = 1e-5),\n",
    "])\n",
    "\n",
    "tuner.tune(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "585db4ce-41a3-4db1-b58a-d6fc85e133ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m createModel(\n\u001b[1;32m      3\u001b[0m     window \u001b[38;5;241m=\u001b[39m       WINDOW_SIZE, \n\u001b[1;32m      4\u001b[0m     min_count \u001b[38;5;241m=\u001b[39m    WORD_MIN_COUNT, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     sample \u001b[38;5;241m=\u001b[39m       SUBSAMPLING_THRESHOLD,\n\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "model = createModel(\n",
    "    window =       WINDOW_SIZE, \n",
    "    min_count =    WORD_MIN_COUNT, \n",
    "    epochs =       EPOCHS_NUMBER, \n",
    "    negative =     NEGATIVE_SAMPLES_AMOUNT,\n",
    "    sample =       SUBSAMPLING_THRESHOLD,\n",
    "    )\n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b47c0d-8840-4cf9-b18c-bd613f73a898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
