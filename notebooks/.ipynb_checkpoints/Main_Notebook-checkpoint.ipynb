{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26212e0c-ae50-44cc-a2d8-059a4bfc5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/trukhinmaksim/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff7897d9-4eb0-45b0-9108-1e25b1533011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['10.22.112.39:27020'], document_class=dict, tz_aware=False, connect=True), 'mini_database'), 'projects')\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# single machine setup (mongo is running here localy)\n",
    "# \"ip a\" for ip address\n",
    "MY_DATABASE_LINK = 'mongodb://10.22.112.39:27020/' #'mongodb://192.168.100.57:27020/'\n",
    "# multiple mechine setup (mongo is running on another machine)\n",
    "#MY_DATABASE_LINK = 'mongodb://192.168.43.78:27020/'\n",
    "\n",
    "class DatabaseConnect:\n",
    "    DB_LINK = MY_DATABASE_LINK\n",
    "\n",
    "    class Base:\n",
    "        client = None\n",
    "        @classmethod\n",
    "        def connect(cls, databaseName):\n",
    "            cls.client = MongoClient(DatabaseConnect.DB_LINK)\n",
    "            # Access the database\n",
    "            return cls.client[databaseName]\n",
    "\n",
    "        @classmethod\n",
    "        def close(cls):\n",
    "            if cls.client:\n",
    "                cls.client.close()\n",
    "                cls.client = None\n",
    "\n",
    "        @classmethod\n",
    "        def getCollection(cls, collectionName):\n",
    "            return cls.client[collectionName]\n",
    "\n",
    "\n",
    "    class mini_database(Base):\n",
    "        @classmethod\n",
    "        def projects(cls):\n",
    "            #print(cls.connect)\n",
    "            \n",
    "            return cls.connect('mini_database')['projects']\n",
    "        @classmethod\n",
    "        def users(cls):\n",
    "            return cls.connect('mini_database')['users']\n",
    "\n",
    "projectsCollection = DatabaseConnect.mini_database.projects()\n",
    "usersCollection = DatabaseConnect.mini_database.users()\n",
    "print(projectsCollection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0493c1d7-05a0-4f37-896f-75282fa4814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd90b42e-421f-42f2-8449-01a3969b52a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned 2000 projects\n",
      "Maximum stars amount: 284894\n",
      "Number of projects, that has more stars than threshold: 476\n",
      "Stars threshold: 2.0\n"
     ]
    }
   ],
   "source": [
    "def findApproximateStarsThreshold(projectsNum : int, percentile=50):\n",
    "    count = projectsNum\n",
    "    cursor = projectsCollection.find()\n",
    "    stars = []\n",
    "\n",
    "    for proj in cursor:\n",
    "        if count <= 0: break\n",
    "        stars.append(proj[\"stars\"])\n",
    "        count -= 1\n",
    "\n",
    "    stars = np.array(stars)\n",
    "    print(f\"Scanned {projectsNum - count} projects\")\n",
    "    print(f\"Maximum stars amount: {stars.max()}\")\n",
    "\n",
    "    threshold = np.percentile(stars, percentile)\n",
    "    print(f\"Number of projects, that has more stars than threshold: {len(stars[stars > threshold])}\")\n",
    "\n",
    "    return threshold\n",
    "\n",
    "# Keep top 30% of projects\n",
    "srarsThreshold = findApproximateStarsThreshold(2000, 75)\n",
    "print(f\"Stars threshold: {srarsThreshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f6e6ba-2fff-4802-ab7b-8277dacd4d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Albanian -> English, Arabic -> English, Azerbaijani -> English, Basque -> English, Bengali -> English, Bulgarian -> English, Catalan -> English, Chinese (traditional) -> English, Chinese -> English, Czech -> English, Danish -> English, Dutch -> English, English -> Albanian, English -> Arabic, English -> Azerbaijani, English -> Basque, English -> Bengali, English -> Bulgarian, English -> Catalan, English -> Chinese, English -> Chinese (traditional), English -> Czech, English -> Danish, English -> Dutch, English -> Esperanto, English -> Estonian, English -> Finnish, English -> French, English -> Galician, English -> German, English -> Greek, English -> Hebrew, English -> Hindi, English -> Hungarian, English -> Indonesian, English -> Irish, English -> Italian, English -> Japanese, English -> Korean, English -> Latvian, English -> Lithuanian, English -> Malay, English -> Norwegian, English -> Persian, English -> Polish, English -> Portuguese, English -> Romanian, English -> Russian, English -> Slovak, English -> Slovenian, English -> Spanish, English -> Swedish, English -> Tagalog, English -> Thai, English -> Turkish, English -> Ukranian, English -> Urdu, Esperanto -> English, Estonian -> English, Finnish -> English, French -> English, Galician -> English, German -> English, Greek -> English, Hebrew -> English, Hindi -> English, Hungarian -> English, Indonesian -> English, Irish -> English, Italian -> English, Japanese -> English, Korean -> English, Latvian -> English, Lithuanian -> English, Malay -> English, Norwegian -> English, Persian -> English, Polish -> English, Portuguese -> English, Portuguese -> Spanish, Romanian -> English, Russian -> English, Slovak -> English, Slovenian -> English, Spanish -> English, Spanish -> Portuguese, Swedish -> English, Tagalog -> English, Thai -> English, Turkish -> English, Ukrainian -> English, Urdu -> English]\n",
      "Chinese â†’ English\n"
     ]
    }
   ],
   "source": [
    "import argostranslate.package\n",
    "import argostranslate.translate\n",
    "\n",
    "argostranslate.package.update_package_index()\n",
    "available_packages = argostranslate.package.get_available_packages()\n",
    "print(available_packages)\n",
    "cn_en_pkg = next(filter(lambda pkg: pkg.from_code == \"zh\" and pkg.to_code == \"en\", available_packages))\n",
    "print(cn_en_pkg)\n",
    "argostranslate.package.install_from_path(cn_en_pkg.download())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e505ce6-03df-4b65-a3c2-ba7e380511ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from copy import deepcopy\n",
    "import re\n",
    "\n",
    "from src.utils.CacheAdapter import JSONAdapter\n",
    "\n",
    "class ProjectsDatasetManager:\n",
    "    def __init__(self, userNumber = float(\"inf\"), validate = lambda data: True, cacheAdapter = None):\n",
    "        self.userNumber = userNumber\n",
    "        self.validate = validate\n",
    "        self.data = None\n",
    "        self.preprocessed = False\n",
    "        self.ignoredUsers = []\n",
    "        \n",
    "        if cacheAdapter == None: \n",
    "            self.cacheAdapter = JSONAdapter()\n",
    "        else:\n",
    "            self.cacheAdapter = cacheAdapter\n",
    "\n",
    "    def ignoreUsers(self, users_ids : list[str]):\n",
    "        self.ignoredUsers.extend(users_ids)\n",
    "    \n",
    "    def fromCache(self):\n",
    "        self.data = self.cacheAdapter.load()\n",
    "\n",
    "        # it is assumed, that cache only contains already preprocessed data\n",
    "        self.preprocessed = True\n",
    "        return self.data\n",
    "\n",
    "    def fromDB(self):\n",
    "        self.data = self.getProjectsDataForUsers()\n",
    "        self.preprocessed = False # assume, that database contains unprocessed data\n",
    "        return self.data\n",
    "\n",
    "    def getProjectsDataForUsers(self) -> dict[str, list]:\n",
    "        # will return a dictionary, where keys are users ids and values are lists of projects ids, each user has contributed to\n",
    "        i = 0\n",
    "        count = self.userNumber\n",
    "        cursor = usersCollection.find()\n",
    "        data = {}\n",
    "\n",
    "        for user in cursor:\n",
    "            if count <= 0: break\n",
    "            if user[\"id\"] in self.ignoredUsers: continue # if that user must be ignored, just skip to the next one\n",
    "            print(f\"Scanning user: {i}\")\n",
    "            projectsIDList = user[\"projects\"]\n",
    "\n",
    "            projects = []\n",
    "\n",
    "            for proj_id in projectsIDList:\n",
    "                projectData = projectsCollection.find_one({\"id\" : proj_id}, {\"_id\" : False})\n",
    "\n",
    "                if self.validate(projectData):\n",
    "                    projects.append(projectData)\n",
    "        \n",
    "            if len(projects):\n",
    "                # if user has at least one project he contributed to\n",
    "                data[user[\"id\"]] = deepcopy(projects)\n",
    "                count -= 1\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return data\n",
    "\n",
    "    def translateText(self, text):\n",
    "        # will try to use Google Translate, but if any error occures, will use Argos offline translator\n",
    "        if text.isascii(): return text\n",
    "\n",
    "        try:\n",
    "            import asyncio\n",
    "            import nest_asyncio\n",
    "\n",
    "            async def inner():\n",
    "                nonlocal text\n",
    "                from googletrans import Translator\n",
    "\n",
    "                async with Translator() as translator:\n",
    "                    result = await translator.translate(text, dest = \"en\")\n",
    "\n",
    "                return result\n",
    "\n",
    "            nest_asyncio.apply()  # Patch the event loop    \n",
    "            return asyncio.run(inner()).text\n",
    "\n",
    "        except Exception as exp:\n",
    "            # assume, that the text is in Chinese and translate it using argos translator\n",
    "            print(f\"Using Argos for {text[:10]}...\")\n",
    "            return argostranslate.translate.translate(text, \"zh\", \"en\")\n",
    "            \"\"\"\n",
    "            if str(type(exp)) == \"<class 'httpx.ConnectError'>\":\n",
    "                return text\n",
    "            else:\n",
    "                raise exp\n",
    "            \"\"\"\n",
    "\n",
    "    def textPreprocessing(self, text):\n",
    "        # Initialize tools\n",
    "        stop_words = set(stopwords.words(\"english\") + [\"etc\"])\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Translate:\n",
    "        #text = self.translateText(text)\n",
    "        # Remove unicode:\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "        # Process camel case:\n",
    "        #text = processCamelCase(text)\n",
    "        # Lower the text:\n",
    "        text = text.lower()\n",
    "        # Remove punctuation:\n",
    "        text = text.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "        # Remove stop-words:\n",
    "        #text = re.sub(\"\\s\" + \"|\".join(stop_words) + \"\\s\", \" \", text)\n",
    "        # Remove numbers:\n",
    "        text = re.sub(r\"\\d\", \" \", text)\n",
    "        # Remove new lines:\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        # Remove multiple spaces:\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "        tokens = [word for word in word_tokenize(text) if word not in stop_words and len(word) > 1]  # Tokenize into words\n",
    "        \n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Remove stopwords & lemmatize\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def projectsDataPreprocessing(self, projects : np.array(dict), including_text : bool = False) -> np.array([{\"tokens\" : str, \"tags\" : list}]):\n",
    "        # will take in an array of projects and prepare it to be consumed by the model\n",
    "        # takes: array of projects (as dictionaries); returns: text data and tags for every project in array\n",
    "        result = []\n",
    "\n",
    "        for proj in projects:\n",
    "            joinedText = \" \".join([proj[\"name\"], proj[\"description\"]])\n",
    "\n",
    "            tockens = self.textPreprocessing(joinedText)\n",
    "            tags = [proj[\"id\"], proj[\"name\"], proj[\"language\"]] + proj[\"topics\"]# if proj[\"language\"] else proj[\"topics\"]\n",
    "            if including_text:\n",
    "                result.append({\"text\" : joinedText, \"tokens\" : tockens, \"tags\" : tags})\n",
    "            else:\n",
    "                result.append({\"tokens\" : tockens, \"tags\" : tags})\n",
    "\n",
    "        return np.array(result)\n",
    "\n",
    "    def preprocess(self, _data : dict | None = None, including_text : bool = False) -> dict[str, list]:\n",
    "        if self.preprocessed: return self.data\n",
    "\n",
    "        if _data:\n",
    "            data = _data\n",
    "        elif self.data:\n",
    "            data = self.data\n",
    "        else:\n",
    "            return self.fromCache()\n",
    "\n",
    "        for user_id, projs in data.items():\n",
    "            #print(type(np.array(userProjs)))\n",
    "            data[user_id] = self.projectsDataPreprocessing(projs, including_text)\n",
    "\n",
    "        self.preprocessed = True\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a2718f-e135-4206-923a-b6c2455a3d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning user: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'deepcopy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m adapter \u001b[38;5;241m=\u001b[39m JSONAdapter()\n\u001b[1;32m      8\u001b[0m manager \u001b[38;5;241m=\u001b[39m ProjectsDatasetManager(USERS_NUMBER_TO_SCAN, projectDataIsSufficient, cacheAdapter \u001b[38;5;241m=\u001b[39m adapter)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromDB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m manager\u001b[38;5;241m.\u001b[39mpreprocess()\n\u001b[1;32m     11\u001b[0m manager\u001b[38;5;241m.\u001b[39mdata\n",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m, in \u001b[0;36mProjectsDatasetManager.fromDB\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfromDB\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetProjectsDataForUsers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# assume, that database contains unprocessed data\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\n",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m, in \u001b[0;36mProjectsDatasetManager.getProjectsDataForUsers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m         projects\u001b[38;5;241m.\u001b[39mappend(projectData)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(projects):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# if user has at least one project he contributed to\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     data[user[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m(projects)\n\u001b[1;32m     55\u001b[0m     count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deepcopy' is not defined"
     ]
    }
   ],
   "source": [
    "def projectDataIsSufficient(projectData):\n",
    "    # used to filter data by quality, for example, I can take only those project, that has long description, readme file and many stars\n",
    "    return (projectData and projectData[\"description\"] and (len(projectData[\"topics\"]) or projectData[\"language\"]))\n",
    "\n",
    "USERS_NUMBER_TO_SCAN = 5\n",
    "\n",
    "adapter = JSONAdapter()\n",
    "manager = ProjectsDatasetManager(USERS_NUMBER_TO_SCAN, projectDataIsSufficient, cacheAdapter = adapter)\n",
    "manager.fromDB()\n",
    "manager.preprocess()\n",
    "manager.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b548fdf-f3f5-4595-9026-9ed2c2b9d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "2/0\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc718e5-d83e-4bfc-a8c2-e676cf5779e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatternData(data : dict[str, list]) -> np.array(dict):\n",
    "    # takes in data in form of dict, where each key is a user id and each value is a list of that user's projects\n",
    "    # returns just flat list of these projects \n",
    "    result = []\n",
    "\n",
    "    for projectsArray in data.values():\n",
    "        for project in projectsArray:\n",
    "            result.append(project)\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "manager.data = flatternData(manager.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f34f45-9fc1-404c-9710-8d59a72910a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentsCorpus = []\n",
    "\n",
    "for project in manager.data:\n",
    "    documentsCorpus.append(TaggedDocument(words=project[\"tokens\"], tags=project[\"tags\"]))\n",
    "\n",
    "documentsCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90db812-d872-4d07-861f-9a813fb43379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model\n",
    "\n",
    "VECTOR_SIZE = 100\n",
    "EPOCH_NUMBER = 10\n",
    "WORD_MIN_AMOUNT = 3\n",
    "WINDOW_SIZE = 7\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size = VECTOR_SIZE, window = WINDOW_SIZE, min_count = WORD_MIN_AMOUNT, epochs = EPOCH_NUMBER)\n",
    "model.build_vocab(documentsCorpus)\n",
    "model.train(documentsCorpus, total_examples = model.corpus_count, epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f2b62-a566-45c0-aa68-be10628fd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.infer_vector(['awesome', 'code', 'streamer', 'list', 'code', 'streamer', 'multiple', 'plataforms', 'like', 'twitch', 'youtube'])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799be099-49c7-4dca-a082-623cc4fe0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2960155-39a0-4bfa-af2c-fe70234ba64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
