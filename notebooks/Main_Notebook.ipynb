{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7897d9-4eb0-45b0-9108-1e25b1533011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['192.168.1.191:27020'], document_class=dict, tz_aware=False, connect=True), 'mini_database'), 'projects')\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# single machine setup (mongo is running here localy)\n",
    "MY_DATABASE_LINK = 'mongodb://192.168.1.191:27020/' #'mongodb://192.168.100.57:27020/'\n",
    "# multiple mechine setup (mongo is running on another machine)\n",
    "#MY_DATABASE_LINK = 'mongodb://192.168.43.78:27020/'\n",
    "\n",
    "class DatabaseConnect:\n",
    "    DB_LINK = MY_DATABASE_LINK\n",
    "\n",
    "    class Base:\n",
    "        client = None\n",
    "        @classmethod\n",
    "        def connect(cls, databaseName):\n",
    "            cls.client = MongoClient(DatabaseConnect.DB_LINK)\n",
    "            # Access the database\n",
    "            return cls.client[databaseName]\n",
    "\n",
    "        @classmethod\n",
    "        def close(cls):\n",
    "            if cls.client:\n",
    "                cls.client.close()\n",
    "                cls.client = None\n",
    "\n",
    "        @classmethod\n",
    "        def getCollection(cls, collectionName):\n",
    "            return cls.client[collectionName]\n",
    "\n",
    "\n",
    "    class mini_database(Base):\n",
    "        @classmethod\n",
    "        def projects(cls):\n",
    "            #print(cls.connect)\n",
    "            \n",
    "            return cls.connect('mini_database')['projects']\n",
    "        @classmethod\n",
    "        def users(cls):\n",
    "            return cls.connect('mini_database')['users']\n",
    "\n",
    "projectsCollection = DatabaseConnect.mini_database.projects()\n",
    "usersCollection = DatabaseConnect.mini_database.users()\n",
    "print(projectsCollection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0493c1d7-05a0-4f37-896f-75282fa4814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3bb257-9cda-4e46-bed9-a7b13cb91b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProjectsStars(projectsNum : int) -> list:\n",
    "    count = projectsNum\n",
    "    cursor = projectsCollection.find()\n",
    "    stars = []\n",
    "\n",
    "    for proj in cursor:\n",
    "        if count <= 0: break\n",
    "        stars.append(proj[\"stars\"])\n",
    "        count -= 1\n",
    "\n",
    "    return np.array(stars)\n",
    "\n",
    "def getProjectsForPeriods(stars : list, periodSize = 50):\n",
    "    class Period(list):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.projectsAmount = 0\n",
    "\n",
    "        def includes(self, n):\n",
    "            return (self[0] <= n) and (self[1] >= n)\n",
    "\n",
    "\n",
    "    maxVal = max(stars)\n",
    "    print(maxVal)\n",
    "    # construct periods:\n",
    "    periods = [Period((i, i + periodSize)) for i in range(0, maxVal + periodSize, periodSize)]\n",
    "    if maxVal % periodSize: periods += [Period((maxVal - (maxVal % periodSize), maxVal))]\n",
    "\n",
    "    print(len(periods))\n",
    "    projectsPerPeriod = [0] * len(periods)\n",
    "    projectsCursor = projectsCollection.find()\n",
    "\n",
    "    for projectStars in stars:\n",
    "        for index, period in enumerate(periods):\n",
    "            if period.includes(projectStars):\n",
    "                period.projectsAmount += 1\n",
    "                #projectsPerPeriod[index] += 1\n",
    "\n",
    "    return {\"periods\" : periods, \"projects_amount\" : projectsPerPeriod}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f984eedc-7707-4aba-8bfd-d33f50837217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284894"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starsArray = getProjectsStars(1000)\n",
    "\n",
    "maximum = max(starsArray)\n",
    "maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8caa7fac-825a-4258-97d0-bb653172a256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 2.0\n",
      "Original size: 1000\n",
      "272\n"
     ]
    }
   ],
   "source": [
    "def percentile_threshold(likes, percentile=50):\n",
    "    return np.percentile(likes, percentile)\n",
    "\n",
    "threshold = percentile_threshold(starsArray, 70)  # Keep top 30% posts\n",
    "\n",
    "print(\"Threshold:\", threshold)\n",
    "print(f\"Original size: {len(starsArray)}\")\n",
    "print(len(starsArray[starsArray > threshold]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e505ce6-03df-4b65-a3c2-ba7e380511ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "class CacheAdapter:\n",
    "    def __init__(self, collectionName = \"\"):\n",
    "        self.collectionName = collectionName\n",
    "\n",
    "    def load(self):\n",
    "        return {}\n",
    "\n",
    "    def save(self, data):\n",
    "        return {}\n",
    "\n",
    "class JSONAdapter(CacheAdapter):\n",
    "    PREPROCESSED_DATA_CACHE_PATH = \"/home/trukhinmaksim/src/mycache\"\n",
    "\n",
    "    @classmethod\n",
    "    def default(cls):\n",
    "        return cls()\n",
    "    \n",
    "    def load(self):\n",
    "        # will load data from JSON file, argument 'collectionName' is a file name\n",
    "        if self.collectionName:\n",
    "            fileName = self.collectionName\n",
    "        else:\n",
    "            # take the first file from the directory:\n",
    "            fileName = next(os.walk(JSONAdapter.PREPROCESSED_DATA_CACHE_PATH))[2][0]\n",
    "\n",
    "        print(fileName)\n",
    "\n",
    "        with open(os.path.join(JSONAdapter.PREPROCESSED_DATA_CACHE_PATH, fileName), encoding = \"utf-8\") as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def save(self, data):\n",
    "        # will write data into the predefined JSON file\n",
    "        if self.collectionName:\n",
    "            fileName = self.collectionName\n",
    "        else:\n",
    "            # take the first file from the directory:\n",
    "            fileName = next(os.walk(JSONAdapter.PREPROCESSED_DATA_CACHE_PATH))[2][0]\n",
    "\n",
    "        with open(os.path.join(JSONAdapter.PREPROCESSED_DATA_CACHE_PATH, fileName), \"w\", encoding = \"utf-8\") as file:\n",
    "            json.dump(usersProjects, fp = file)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class ProjectsDatasetManager:\n",
    "    def __init__(self, userNumber = float(\"inf\"), validate = lambda data: True, cacheAdapter = None):\n",
    "        self.userNumber = userNumber\n",
    "        self.validate = validate\n",
    "        self.data = None\n",
    "        self.preprocessed = False\n",
    "\n",
    "        if cacheAdapter == None: \n",
    "            self.cacheAdapter = JSONAdapter()\n",
    "        else:\n",
    "            self.cacheAdapter = cacheAdapter\n",
    "\n",
    "    def fromCache(self):\n",
    "        self.data = self.cacheAdapter.load()\n",
    "\n",
    "        # it is assumed, that cache only contains already preprocessed data\n",
    "        self.preprocessed = True\n",
    "        return self.data\n",
    "\n",
    "    def fromDB(self):\n",
    "        self.data = self.getProjectsDataForUsers()\n",
    "        self.preprocessed = False # assume, that database contains unprocessed data\n",
    "        return self.data\n",
    "\n",
    "    def getProjectsDataForUsers(self) -> dict[str, list]:\n",
    "        # will return a dictionary, where keys are users ids and values are lists of projects ids, each user has contributed to\n",
    "        i = 0\n",
    "        count = self.userNumber\n",
    "        cursor = usersCollection.find()\n",
    "        data = {}\n",
    "\n",
    "        for user in cursor:\n",
    "            if count <= 0: break\n",
    "            print(f\"Scanning user: {i}\")\n",
    "            projectsIDList = user[\"projects\"]\n",
    "\n",
    "            projects = []\n",
    "\n",
    "            for proj_id in projectsIDList:\n",
    "                projectData = projectsCollection.find_one({\"id\" : proj_id}, {\"_id\" : False})\n",
    "\n",
    "                if self.validate(projectData):\n",
    "                    projects.append(projectData)\n",
    "        \n",
    "            if len(projects):\n",
    "                data[user[\"id\"]] = deepcopy(projects)\n",
    "                count -= 1\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return data\n",
    "\n",
    "    def translateText(self, text):\n",
    "        if text.isascii(): return text\n",
    "    \n",
    "        try:\n",
    "            import asyncio\n",
    "            import nest_asyncio\n",
    "\n",
    "            async def inner():\n",
    "                nonlocal text\n",
    "                from googletrans import Translator\n",
    "\n",
    "                async with Translator() as translator:\n",
    "                    result = await translator.translate(text, dest = \"en\")\n",
    "\n",
    "                return result\n",
    "\n",
    "            nest_asyncio.apply()  # Patch the event loop    \n",
    "            return asyncio.run(inner()).text\n",
    "\n",
    "        except Exception as exp:\n",
    "            if str(type(exp)) == \"<class 'httpx.ConnectError'>\":\n",
    "                return text\n",
    "            else:\n",
    "                raise exp\n",
    "\n",
    "    def textPreprocessing(self, text):\n",
    "        # Initialize tools\n",
    "        stop_words = set(stopwords.words(\"english\") + [\"etc\"])\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Translate:\n",
    "        text = self.translateText(text)\n",
    "        # Remove unicode:\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "        # Process camel case:\n",
    "        #text = processCamelCase(text)\n",
    "        # Lower the text:\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "        # Remove stop-words:\n",
    "        #text = re.sub(\"\\s\" + \"|\".join(stop_words) + \"\\s\", \" \", text)\n",
    "        # Remove new lines\n",
    "        text = re.sub(r\"\\n\", \" \", text)\n",
    "        # Remove multiple spaces:\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "        tokens = [word for word in word_tokenize(text) if not word in stop_words]  # Tokenize into words\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Remove stopwords & lemmatize\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def projectsDataPreprocessing(self, projects : np.array(dict)) -> np.array([{\"tokens\" : str, \"tags\" : list}]):\n",
    "        # will take in an array of projects and prepare it to be consumed by the model\n",
    "        # takes: array of projects (as dictionaries); returns: text data and tags for every project in array\n",
    "        result = []\n",
    "\n",
    "        for proj in projects:\n",
    "            tockens = self.textPreprocessing(\" \".join([proj[\"name\"], proj[\"description\"]]))\n",
    "            tags = [proj[\"id\"], proj[\"name\"], proj[\"language\"]] + proj[\"topics\"]# if proj[\"language\"] else proj[\"topics\"]\n",
    "            result.append({\"tokens\" : tockens, \"tags\" : tags})\n",
    "\n",
    "        return np.array(result)\n",
    "\n",
    "    def preprocess(self, _data : dict | None = None) -> dict:\n",
    "        if self.preprocessed: return self.data\n",
    "\n",
    "        if _data:\n",
    "            data = _data\n",
    "        elif self.data:\n",
    "            data = self.data\n",
    "        else:\n",
    "            return self.fromCache()\n",
    "\n",
    "        for user_id, projs in data.items():\n",
    "            #print(type(np.array(userProjs)))\n",
    "            data[user_id] = self.projectsDataPreprocessing(projs)\n",
    "\n",
    "        self.preprocessed = True\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a2718f-e135-4206-923a-b6c2455a3d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning user: 0\n",
      "Scanning user: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "maketrans() argument 2 must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m manager \u001b[38;5;241m=\u001b[39m ProjectsDatasetManager(\u001b[38;5;241m2\u001b[39m, projectDataIsSufficient, cacheAdapter \u001b[38;5;241m=\u001b[39m adapter)\n\u001b[1;32m      6\u001b[0m manager\u001b[38;5;241m.\u001b[39mfromDB()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m manager\u001b[38;5;241m.\u001b[39mdata\n",
      "Cell \u001b[0;32mIn[8], line 183\u001b[0m, in \u001b[0;36mProjectsDatasetManager.preprocess\u001b[0;34m(self, _data)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfromCache()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_id, projs \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m#print(type(np.array(userProjs)))\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     data[user_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojectsDataPreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprojs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn[8], line 165\u001b[0m, in \u001b[0;36mProjectsDatasetManager.projectsDataPreprocessing\u001b[0;34m(self, projects)\u001b[0m\n\u001b[1;32m    162\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proj \u001b[38;5;129;01min\u001b[39;00m projects:\n\u001b[0;32m--> 165\u001b[0m     tockens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextPreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     tags \u001b[38;5;241m=\u001b[39m [proj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], proj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], proj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m+\u001b[39m proj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;66;03m# if proj[\"language\"] else proj[\"topics\"]\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m : tockens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m : tags})\n",
      "Cell \u001b[0;32mIn[8], line 146\u001b[0m, in \u001b[0;36mProjectsDatasetManager.textPreprocessing\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    144\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28;43mstr\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaketrans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpunctuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpunctuation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Remove stop-words:\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m#text = re.sub(\"\\s\" + \"|\".join(stop_words) + \"\\s\", \" \", text)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Remove new lines\u001b[39;00m\n\u001b[1;32m    150\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mTypeError\u001b[0m: maketrans() argument 2 must be str, not list"
     ]
    }
   ],
   "source": [
    "def projectDataIsSufficient(projectData):\n",
    "    return (projectData and projectData[\"description\"] and (len(projectData[\"topics\"]) or projectData[\"language\"]))\n",
    "\n",
    "adapter = JSONAdapter()\n",
    "manager = ProjectsDatasetManager(2, projectDataIsSufficient, cacheAdapter = adapter)\n",
    "manager.fromDB()\n",
    "manager.preprocess()\n",
    "manager.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc718e5-d83e-4bfc-a8c2-e676cf5779e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "#from gensim.models.doc2vec import TaggedDocument\n",
    "#from nltk.tokenize import word_tokenize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
